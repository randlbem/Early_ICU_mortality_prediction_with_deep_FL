{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "if 'init_done' in globals():\n",
    "    matplotlib.use(\"pgf\")\n",
    "    matplotlib.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        'font.family': 'serif',\n",
    "        'text.usetex': True,\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "init_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "WINDOW_LENGTH = 24\n",
    "MIN_LOS_ICU = 24\n",
    "N_CLIENTS = [2,4,8]\n",
    "MAX_ROUNDS = 100\n",
    "PATIENCE = 30\n",
    "FOLDER_SUFFIXES = {'':'loss'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load result files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = {suffix:{'cml':{}, 'fl':{}, 'lml':{}} for suffix in FOLDER_SUFFIXES}\n",
    "scores_valid = {suffix:{'cml':{}, 'fl':{}, 'lml':{}} for suffix in FOLDER_SUFFIXES}\n",
    "scores_test =  {suffix:{'cml':{}, 'fl':{}, 'lml':{}} for suffix in FOLDER_SUFFIXES}\n",
    "predictions =  {suffix:{'cml':{}, 'fl':{}, 'lml':{}} for suffix in FOLDER_SUFFIXES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(n_clients, fl=False):\n",
    "    for suffix in FOLDER_SUFFIXES:\n",
    "        path =  f'../scores/min{MIN_LOS_ICU:d}h/{WINDOW_LENGTH:d}h{suffix:s}/'\n",
    "        path += ('scores_fl_' if fl else 'scores_')\n",
    "        path += f'{n_clients:d}clients_{WINDOW_LENGTH:d}h(min{MIN_LOS_ICU:d}h).pickle'\n",
    "\n",
    "        print(f'Loading file \"{path:s}\"', end='...')\n",
    "\n",
    "        key = 'cml' if n_clients == 1 else 'fl' if fl else 'lml'\n",
    "\n",
    "        try:\n",
    "            with open(path, 'rb') as file:\n",
    "                scores_train[suffix][key][n_clients], scores_valid[suffix][key][n_clients], scores_test[suffix][key][n_clients], predictions[suffix][key][n_clients] = pickle.load(file)\n",
    "        except:\n",
    "            with open(path, 'rb') as file:\n",
    "                scores_train[suffix][key][n_clients], scores_valid[suffix][key][n_clients], scores_test[suffix][key][n_clients] = pickle.load(file)\n",
    "\n",
    "        print(f'Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load central scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(n_clients=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FL-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in N_CLIENTS:\n",
    "    load(n_clients=n, fl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in N_CLIENTS:\n",
    "    load(n_clients=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate scores with sk-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assets\n",
    "from helpers import enumerate_predictions\n",
    "n_labels = 2\n",
    "n_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in predictions:\n",
    "    for model in predictions[suffix]:\n",
    "        for n_clients in predictions[suffix][model]:\n",
    "            # Init scores-arrays:\n",
    "            scores_test[suffix][model][n_clients]['AUROC'] = np.zeros((n_fold, n_labels))\n",
    "            scores_test[suffix][model][n_clients]['AUPRC'] = np.zeros((n_fold, n_labels))\n",
    "            scores_test[suffix][model][n_clients]['precision'] = np.zeros((n_fold, n_labels))\n",
    "            scores_test[suffix][model][n_clients]['recall'] = np.zeros((n_fold, n_labels))\n",
    "            \n",
    "            # Calculate actual number of scores:\n",
    "            n = 1 if model=='fl' else n_clients\n",
    "            \n",
    "            for fold in range(n_fold):\n",
    "                f = 1. / float(n) \n",
    "                for i in range(n):\n",
    "                    y_true = []\n",
    "                    y_pred = []\n",
    "                    for t, p in enumerate_predictions(predictions[suffix][model][n_clients], n_labels=n_labels, client=i, fold=fold):\n",
    "                        y_true.append(t.astype(int))\n",
    "                        y_pred.append(p.astype(float))\n",
    "                    y_true = np.array(y_true)\n",
    "                    y_pred = np.array(y_pred)\n",
    "\n",
    "                    # Calculate classification metrics:\n",
    "                    for label in range(n_labels):\n",
    "                        prc_crv, rcl_crv, _ = sklearn.metrics.precision_recall_curve(y_true[:, label], y_pred[:, label])\n",
    "\n",
    "                        scores_test[suffix][model][n_clients]['AUROC'][fold, label]     += f * sklearn.metrics.roc_auc_score(y_true[:, label], y_pred[:, label])\n",
    "                        scores_test[suffix][model][n_clients]['AUPRC'][fold, label]     += f * sklearn.metrics.auc(rcl_crv, prc_crv)\n",
    "                        scores_test[suffix][model][n_clients]['precision'][fold, label] += f * sklearn.metrics.precision_score(y_true[:, label], np.round(y_pred[:, label]))\n",
    "                        scores_test[suffix][model][n_clients]['recall'][fold, label]    += f * sklearn.metrics.recall_score(y_true[:, label], np.round(y_pred[:, label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate F1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_f1(scores):\n",
    "    precision = scores['precision']\n",
    "    recall = scores['recall']\n",
    "\n",
    "    scores['F1'] = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in scores_train:\n",
    "    for model in scores_train[suffix]:\n",
    "        for n_clients in scores_train[suffix][model]:\n",
    "            add_f1(scores_train[suffix][model][n_clients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in scores_valid:\n",
    "    for model in scores_valid[suffix]:\n",
    "        for n_clients in scores_valid[suffix][model]:\n",
    "            add_f1(scores_valid[suffix][model][n_clients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in scores_test:\n",
    "    for model in scores_test[suffix]:\n",
    "        for n_clients in scores_test[suffix][model]:\n",
    "            add_f1(scores_test[suffix][model][n_clients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create latex table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUROC', 'AUPRC', 'F1', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in FOLDER_SUFFIXES:\n",
    "    print('============================================================')\n",
    "    print(f'Suffix: \"{suffix:s}\"')\n",
    "    print('------------------------------------------------------------\\n')\n",
    "    \n",
    "    table = ''\n",
    "    for m in metrics:\n",
    "        # Row title:\n",
    "        table += f'\\\\head\\u007B{m:s}\\u007D\\t'\n",
    "\n",
    "        # Central model score:\n",
    "        avg = np.nanmean(scores_test[suffix]['cml'][1][m][:,1:])\n",
    "        std = np.nanstd(scores_test[suffix]['cml'][1][m][:,1:])\n",
    "        table += f'& ${avg:.2f} \\\\pm {std:.2f}$\\t'\n",
    "\n",
    "        # FL and local model scores:\n",
    "        for model in ['fl', 'lml']:\n",
    "            for n_clients in N_CLIENTS:\n",
    "                avg = np.nanmean(scores_test[suffix][model][n_clients][m][:,1:])\n",
    "                std = np.nanstd(scores_test[suffix][model][n_clients][m][:,1:])\n",
    "                table += f'& ${avg:.2f} \\\\pm {std:.2f}$\\t'\n",
    "\n",
    "        table += '\\\\\\\\\\n'\n",
    "\n",
    "    print(table)\n",
    "print('============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_curve(scores,  color, label, ax, client=None, round=None, step=1, bounds=None):\n",
    "    # extract and reshape y-values:\n",
    "    y = None\n",
    "    if client != None:\n",
    "        if round != None:   y = scores[round, client].reshape((1, scores.shape[-1])).copy()\n",
    "        else:               y = scores[:, client].copy()\n",
    "\n",
    "    else:\n",
    "        if round != None:   y = scores[round, :].copy()\n",
    "        else:               y = scores.reshape((np.prod(scores.shape[:-1]), scores.shape[-1])).copy()\n",
    "\n",
    "    # find value counts and fill curves with last value:\n",
    "    counts = np.empty(y.shape[0], dtype='int')\n",
    "    for i in range(y.shape[0]):\n",
    "        last_value = np.nan\n",
    "\n",
    "        for j in range(y.shape[1]):\n",
    "            if np.isnan(y[i,j]):\n",
    "                y[i,j] = last_value\n",
    "            \n",
    "            else:\n",
    "                last_value = y[i,j]\n",
    "                counts[i] = j + 1\n",
    "\n",
    "    # calculate x-values:\n",
    "    n = np.max(counts)\n",
    "    x = np.arange(1,(n*step)+1,step)\n",
    "    \n",
    "    # plot curve:\n",
    "    ax.plot(x[:n], np.mean(y, axis=0)[:n], color=color, label=label)\n",
    "\n",
    "    if len(counts) > 1:\n",
    "        if bounds == 'mean_std':\n",
    "            # Plot standard deviation:\n",
    "            y_avg = np.mean(y, axis=0)\n",
    "            ax.fill_between(\n",
    "                x,\n",
    "                np.nanmean(np.where(y < y_avg, y, np.nan), axis=0)[:n],\n",
    "                np.nanmean(np.where(y > y_avg, y, np.nan), axis=0)[:n],\n",
    "                color=color,\n",
    "                alpha=.25\n",
    "            )\n",
    "\n",
    "        if bounds == 'min_max':\n",
    "            # Plot min and max:\n",
    "            y_min = np.min(y, axis=0)[:n]\n",
    "            y_max = np.max(y, axis=0)[:n]\n",
    "            ax.fill_between(x, y_min, y_max, color=color, alpha=.25)\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_best_weights(scores, color, label, ax, client=None, round=None, bounds=None):\n",
    "    # find value counts:\n",
    "    counts = np.empty(scores.shape[0:2], dtype='int')\n",
    "    for i in [round] if round != None else range(scores.shape[0]):\n",
    "        for j in [client] if client != None else range(scores.shape[1]):\n",
    "            for n in range(scores.shape[2]):\n",
    "                if not np.isnan(scores[i,j,n]):\n",
    "                    counts[i,j] = n + 1\n",
    "\n",
    "    x = counts[counts < MAX_ROUNDS].flatten() - PATIENCE\n",
    "    y_lims = (0, ax.get_ylim()[1])\n",
    "\n",
    "    if bounds == 'mean_std':\n",
    "        # Print best weights with mean and standard deviation:\n",
    "        x_avg = np.mean(x)\n",
    "        ax.axvline(x=x_avg, color=color, linestyle='--', label=label)\n",
    "\n",
    "        if len(x) > 1:\n",
    "            ax.fill_betweenx(y_lims, np.mean(x[x < x_avg]), np.mean(x[x > x_avg]), color=color, alpha=.25)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Print best weights with min and max:\n",
    "        x_min = np.min(x)\n",
    "        ax.axvline(x=x_min, color=color, linestyle='--', label=label)\n",
    "\n",
    "        if len(x) > 1:\n",
    "            x_max = np.max(x)\n",
    "            ax.axvline(x=x_max, color=color, linestyle='--')\n",
    "            ax.fill_betweenx(y_lims, x_min, x_max, color=color, alpha=.25)\n",
    "\n",
    "    ax.set_ylim(y_lims)\n",
    "\n",
    "    return np.max(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_plot(metric, n_clients=1, fl=False, suffix=''):\n",
    "    n_rounds = 5\n",
    "\n",
    "    fig = plt.figure(figsize=(2*n_rounds, 1.5*n_clients))\n",
    "\n",
    "    for i in range(n_rounds):\n",
    "        for j in range(n_clients):\n",
    "            ax = fig.add_subplot(n_clients, n_rounds, j*n_rounds + i + 1)\n",
    "\n",
    "            # Choose model:\n",
    "            model = 'fl' if fl else 'lml'\n",
    "            if n_clients==1:\n",
    "                model = 'cml'\n",
    "\n",
    "            # Print curves:\n",
    "            add_curve(scores_train[suffix][model][n_clients][metric], '#3465a4', 'train', ax, client=j, round=i, bounds=None)\n",
    "            add_curve(scores_valid[suffix][model][n_clients][metric], '#f37500', 'valid', ax, client=j, round=i, bounds=None)\n",
    "\n",
    "            # Print best weights:\n",
    "            n = add_best_weights(scores_train[suffix][model][n_clients][metric], '#f10d0c', 'best', ax, client=j, bounds='mean_std')\n",
    "\n",
    "            ax.set_xticks(np.arange(n+1, step=10))\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(metric)\n",
    "\n",
    "            if j == 0:\n",
    "                ax.set_title(f'CV-iteration {i+1:d}')\n",
    "                \n",
    "            elif j == n_clients-1:\n",
    "                ax.set_xlabel('fl-round' if fl else 'epoch')\n",
    "\n",
    "            if i == n_rounds-1 and j == n_clients-1:\n",
    "                ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_plot('loss', n_clients=1, fl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_plot(metric, ax, n_clients=1, client=None, round=None, fl=False, step=1, legend=True, suffixes=FOLDER_SUFFIXES):\n",
    "    colors = [\n",
    "        '#069a2E', #(  6, 154,  46)\n",
    "        '#780373', #(120,   3, 115)\n",
    "        '#f10d0c'  #(241,  13,  12)\n",
    "    ]\n",
    "\n",
    "    # Choose model:\n",
    "    model = 'fl' if fl else 'lml'\n",
    "    if n_clients==1:\n",
    "        model = 'cml'\n",
    "\n",
    "    # Print curves:\n",
    "    n = max(\n",
    "        add_curve(np.concatenate([scores_train[suffix][model][n_clients][metric] for suffix in suffixes]), '#3465a4', 'training', ax, client=client, round=round, bounds='mean_std'),\n",
    "        add_curve(np.concatenate([scores_valid[suffix][model][n_clients][metric] for suffix in suffixes]), '#f37500', 'validation', ax, client=client, round=round, bounds='mean_std')\n",
    "    )\n",
    "\n",
    "    # Print best weights:\n",
    "    for suffix in suffixes:\n",
    "        n = max(n, add_best_weights(scores_train[suffix][model][n_clients][metric], colors.pop(), 'best '+suffixes[suffix], ax, client=client, bounds='min_max'))\n",
    "\n",
    "    ax.set_xticks(np.arange(n+1, step=step))\n",
    "\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('FL-round' if fl else 'epoch')\n",
    "    if legend: ax.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = None\n",
    "fl = False\n",
    "n_clients=1\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "learning_plot('loss',      fig.add_subplot(2, 2, 1), step=5, client=client, round=round, fl=fl, n_clients=n_clients, legend=False)\n",
    "learning_plot('F1',        fig.add_subplot(2, 2, 3), step=5, client=client, round=round, fl=fl, n_clients=n_clients, legend=False)\n",
    "\n",
    "learning_plot('precision', fig.add_subplot(2, 2, 2), step=5, client=client, round=round, fl=fl, n_clients=n_clients, legend=False)\n",
    "learning_plot('recall',    fig.add_subplot(2, 2, 4), step=5, client=client, round=round, fl=fl, n_clients=n_clients)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/learning_curve.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = None\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "learning_plot('loss', ax, step=10, client=client, round=round, fl=False, n_clients=1, legend=False)\n",
    "ax.set_title('loss CML')\n",
    "#ax.set_ylim(.0,.8)\n",
    "#ax.set_yticks([.0,.2,.4,.6,.8])\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "learning_plot('loss', ax, step=10, client=client, round=round, fl=True, n_clients=N_CLIENTS[-1], legend=True)\n",
    "ax.set_title(f'loss FL {N_CLIENTS[-1]:d} clients')\n",
    "#ax.set_ylim(.0,.8)\n",
    "#ax.set_yticks([.0,.2,.4,.6,.8])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/learning_curve_cont.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores / Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cml = True\n",
    "plot_fl_clients = [N_CLIENTS[-1]]\n",
    "plot_lml_clients = [N_CLIENTS[-1]]\n",
    "\n",
    "plot_train = False\n",
    "plot_valid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(metric, ax, client=None, round=None, step=1, legend=True, y_ticks=[], suffixes=FOLDER_SUFFIXES):\n",
    "    n = 0\n",
    "    colors = [\n",
    "        '#f37500', #(234, 117,   0)\n",
    "        '#069a2E', #(  6, 154,  46)\n",
    "        '#3465a4', #( 52, 101, 164)\n",
    "        '#780373', #(120,   3, 115)\n",
    "        '#f10d0c'  #(241,  13,  12)\n",
    "    ]\n",
    "\n",
    "    # Plot curves:\n",
    "    if plot_cml:\n",
    "        if plot_train: n = max(n, add_curve(np.concatenate([scores_train[suffix]['cml'][1][metric] for suffix in suffixes]), colors.pop(), 'CML train.', ax, round=round))\n",
    "        if plot_valid: n = max(n, add_curve(np.concatenate([scores_valid[suffix]['cml'][1][metric] for suffix in suffixes]), colors.pop(), 'CML valid.', ax, round=round))\n",
    "\n",
    "    for n_clients in plot_fl_clients:\n",
    "        if plot_train: n = max(n, add_curve(np.concatenate([scores_train[suffix]['fl'][n_clients][metric] for suffix in suffixes]), colors.pop(), f'FL train. ({n_clients:d} cl.)', ax, client=client, round=round, step=1))\n",
    "        if plot_valid: n = max(n, add_curve(np.concatenate([scores_valid[suffix]['fl'][n_clients][metric] for suffix in suffixes]), colors.pop(), f'FL valid. ({n_clients:d} cl.)', ax, client=client, round=round, step=1))\n",
    "        \n",
    "    for n_clients in plot_lml_clients:\n",
    "        if plot_train: n = max(n, add_curve(np.concatenate([scores_train[suffix]['lml'][n_clients][metric] for suffix in suffixes]), colors.pop(), f'LML train. ({n_clients:d} cl.)', ax, client=client, round=round))\n",
    "        if plot_valid: n = max(n, add_curve(np.concatenate([scores_valid[suffix]['lml'][n_clients][metric] for suffix in suffixes]), colors.pop(), f'LML valid. ({n_clients:d} cl.)', ax, client=client, round=round))\n",
    "\n",
    "    ax.set_xticks(np.arange(n+1, step=step))\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_title(metric)\n",
    "    if len(y_ticks) > 0: ax.set_yticks(y_ticks)\n",
    "    if legend: ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = None\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "plot('AUROC', fig.add_subplot(1, 3, 1), step=10, y_ticks=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0], client=client, round=round)\n",
    "plot('AUPRC', fig.add_subplot(1, 3, 2), step=10, y_ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], client=client, round=round, legend=False)\n",
    "plot('F1',    fig.add_subplot(1, 3, 3), step=10, y_ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], client=client, round=round, legend=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/score_curves.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "\n",
    "y_test_cml = pd.DataFrame(np.concatenate([predictions[suffix]['cml'][1][0,0,:,:] for suffix in FOLDER_SUFFIXES]))\n",
    "\n",
    "y_test_fl = {}\n",
    "y_test_lml = {}\n",
    "for n in N_CLIENTS:\n",
    "    y_test_fl[n] = pd.DataFrame(np.concatenate([predictions[suffix]['fl'][n][0,0,:,:] for suffix in FOLDER_SUFFIXES])).dropna()\n",
    "    y_test_lml[n] = pd.DataFrame(np.concatenate([predictions[suffix]['lml'][n][0,0,:,:] for suffix in FOLDER_SUFFIXES])).dropna()\n",
    "\n",
    "for f in range(1,5):\n",
    "    y_test_cml = y_test_cml.append(pd.DataFrame(np.concatenate([predictions[suffix]['cml'][1][f,0,:,:] for suffix in FOLDER_SUFFIXES]))).dropna()\n",
    "\n",
    "    for n in N_CLIENTS:\n",
    "        y_test_fl[n] = y_test_fl[n].append(pd.DataFrame(np.concatenate([predictions[suffix]['fl'][n][f,0,:,:] for suffix in FOLDER_SUFFIXES]))).dropna()\n",
    "        y_test_lml[n] = y_test_lml[n].append(pd.DataFrame(np.concatenate([predictions[suffix]['lml'][n][f,0,:,:] for suffix in FOLDER_SUFFIXES]))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_fl[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    '#f37500', #(234, 117,   0)\n",
    "    '#069a2E', #(  6, 154,  46)\n",
    "    '#3465a4', #( 52, 101, 164)\n",
    "    '#780373', #(120,   3, 115)\n",
    "    '#f10d0c'  #(241,  13,  12)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(model, n_clients, labels, predictions, ax, **kwargs):\n",
    "    labels = labels.round(decimals=0, out=None).astype(int)\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    #auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
    "    auc = np.nanmean(np.concatenate([scores_test[suffix][model][n_clients]['AUROC'][:,1:] for suffix in FOLDER_SUFFIXES]))\n",
    "    score = model.upper()\n",
    "    if n_clients > 1:\n",
    "        score += ' %d cl.' % (n_clients)\n",
    "    score += ' AUC=%.2f' % (auc)\n",
    "    ax.plot(fp, tp, label=score,  **kwargs)\n",
    "    ax.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax.set_xlim(-.05, 1.05)\n",
    "    ax.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax.set_ylim(-.05, 1.05)\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prc(model, n_clients, labels, predictions, ax, **kwargs):\n",
    "    labels = labels.round(decimals=0, out=None).astype(int)\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
    "    #auc = sklearn.metrics.auc(recall, precision)\n",
    "    auc = np.nanmean(np.concatenate([scores_test[suffix][model][n_clients]['AUPRC'][:,1:] for suffix in FOLDER_SUFFIXES]))\n",
    "    score = model.upper()\n",
    "    if n_clients > 1:\n",
    "        score += ' %d cl.' % (n_clients)\n",
    "    score += ' AUC=%.2f' % (auc)\n",
    "    ax.plot(recall, precision, label=score, linewidth=2, **kwargs)\n",
    "    ax.set_title('Precision-Recall (PR) Curve')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_xlim(-.05, 1.05)\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim(-.05, 1.05)\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "# plot AUROC:\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "i_color = 0\n",
    "\n",
    "plot_roc('cml', 1, y_test_cml[0], y_test_cml[1], ax, color=colors[i_color])\n",
    "i_color += 1\n",
    "\n",
    "if len(N_CLIENTS) > 0:\n",
    "    n = N_CLIENTS[-1]\n",
    "    plot_roc('fl', n, y_test_fl[n][0], y_test_fl[n][1], ax, color=colors[i_color])\n",
    "    plot_roc('lml', n, y_test_lml[n][0], y_test_lml[n][1], ax, color=colors[i_color + 1])\n",
    "    i_color += 2\n",
    "\n",
    "no_skill = len(y_test_cml[0][y_test_cml[0]==1]) / len(y_test_cml[0])\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', label='baseline')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# plot AUPRC:\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "i_color = 0\n",
    "\n",
    "plot_prc('cml', 1, y_test_cml[0], y_test_cml[1], ax, color=colors[i_color])\n",
    "i_color += 1\n",
    "\n",
    "if len(N_CLIENTS) > 0:\n",
    "    n = N_CLIENTS[-1]\n",
    "    plot_prc('fl', n, y_test_fl[n][0], y_test_fl[n][1], ax, color=colors[i_color])\n",
    "    plot_prc('lml', n, y_test_lml[n][0], y_test_lml[n][1], ax, color=colors[i_color + 1])\n",
    "    i_color += 2\n",
    "\n",
    "no_skill = len(y_test_cml[0][y_test_cml[0]==1]) / len(y_test_cml[0])\n",
    "ax.plot([0, 1], [no_skill, no_skill], linestyle='--', label='baseline')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# save plot:\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/auc_curves.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "09804e24ad6773f4299ff941abdb533da0618f58a933eb5ec00c0e9780539224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

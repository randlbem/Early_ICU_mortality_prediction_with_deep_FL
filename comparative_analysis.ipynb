{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "if 'init_done' in globals():\n",
    "    matplotlib.use(\"pgf\")\n",
    "    matplotlib.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        'font.family': 'serif',\n",
    "        'text.usetex': True,\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "init_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "MAX_ROUNDS = 100\n",
    "PATIENCE = 30\n",
    "X_KEY = 'window_length'\n",
    "\n",
    "GROUPS = [\n",
    "  { # Group 1:\n",
    "    'name':             'ES: min. loss (fl, 8 clients)',\n",
    "    'n_clients':        [8],\n",
    "    'window_length':    [8,16,24],\n",
    "    'min_los_icu':      24,\n",
    "    'fl':               True,\n",
    "    'folder_suffix':    ''\n",
    "  },\n",
    "  { # Group 2:\n",
    "    'name':             'ES: max. F1 (fl, 8 clients)',\n",
    "    'n_clients':        [8],\n",
    "    'window_length':    [8,16,24],\n",
    "    'min_los_icu':      24,\n",
    "    'fl':               True,\n",
    "    'folder_suffix':    '_ES-F1'\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(group, n, l):\n",
    "    # Create data path:\n",
    "    path =  f'./scores/min{group[\"min_los_icu\"]:d}h/{l:d}h{group[\"folder_suffix\"]:s}/'\n",
    "    path += ('scores_fl_' if group['fl'] and n != 1 else 'scores_')\n",
    "    path += f'{n:d}clients_{l:d}h(min{group[\"min_los_icu\"]:d}h).pickle'\n",
    "\n",
    "    print(f'Loading file \"{path:s}\"', end='...')\n",
    "\n",
    "    # Add subdictionaries to data-tree if necessary:\n",
    "    if not 'scores' in  group:\n",
    "        group['scores'] = {'train':{}, 'valid':{}, 'test':{}}\n",
    "        \n",
    "    if not 'predictions' in group:\n",
    "        group['predictions'] = {}\n",
    "\n",
    "    # Select key:\n",
    "    key = None\n",
    "    if X_KEY == 'n_clients':\n",
    "        key = n\n",
    "        \n",
    "    elif X_KEY == 'window_length':\n",
    "        key = l\n",
    "\n",
    "    # Load data:\n",
    "    try:\n",
    "        with open(path, 'rb') as file:\n",
    "            group['scores']['train'][key], group['scores']['valid'][key], group['scores']['test'][key], group['predictions'][key] = pickle.load(file)\n",
    "    except:\n",
    "        with open(path, 'rb') as file:\n",
    "            group['scores']['train'][key], group['scores']['valid'][key], group['scores']['test'][key] = pickle.load(file)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in GROUPS:\n",
    "    # Sort lists:\n",
    "    group['n_clients'].sort()\n",
    "    group['window_length'].sort()\n",
    "\n",
    "    # Load data:\n",
    "    for n in group['n_clients']:\n",
    "        for l in group['window_length']:\n",
    "            load(group, n, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate Scores with sk-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import enumerate_predictions\n",
    "n_labels = 2\n",
    "n_fold = 5\n",
    "\n",
    "def recalculate_scores(group, key):\n",
    "    # Init scores-arrays:\n",
    "        group['scores']['test'][key]['AUROC'] = np.zeros((n_fold, n_labels))\n",
    "        group['scores']['test'][key]['AUPRC'] = np.zeros((n_fold, n_labels))\n",
    "        group['scores']['test'][key]['F1'] = np.zeros((n_fold, n_labels))\n",
    "        group['scores']['test'][key]['precision'] = np.zeros((n_fold, n_labels))\n",
    "        group['scores']['test'][key]['recall'] = np.zeros((n_fold, n_labels))\n",
    "        \n",
    "        # Calculate actual number of scores:\n",
    "        n = None\n",
    "        if group['fl']:\n",
    "            n = 1\n",
    "\n",
    "        elif X_KEY == 'n_clients':\n",
    "            n = key\n",
    "\n",
    "        else:\n",
    "            n = group['n_clients'][-1]\n",
    "        \n",
    "        for fold in range(n_fold):\n",
    "            f = 1. / float(n) \n",
    "            for i in range(n):\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "                for t, p in enumerate_predictions(group['predictions'][key], n_labels=n_labels, client=i, fold=fold):\n",
    "                    y_true.append(t.astype(int))\n",
    "                    y_pred.append(p.astype(float))\n",
    "                y_true = np.array(y_true)\n",
    "                y_pred = np.array(y_pred)\n",
    "\n",
    "                # Calculate classification metrics:\n",
    "                for label in range(n_labels):\n",
    "                    prc_crv, rcl_crv, _ = sklearn.metrics.precision_recall_curve(y_true[:, label], y_pred[:, label])\n",
    "                    precision = sklearn.metrics.precision_score(y_true[:, label], np.round(y_pred[:, label]))\n",
    "                    recall = sklearn.metrics.recall_score(y_true[:, label], np.round(y_pred[:, label]))\n",
    "\n",
    "                    group['scores']['test'][key]['AUROC'][fold, label] += f * sklearn.metrics.roc_auc_score(y_true[:, label], y_pred[:, label])\n",
    "                    group['scores']['test'][key]['AUPRC'][fold, label] += f * sklearn.metrics.auc(rcl_crv, prc_crv)\n",
    "                    group['scores']['test'][key]['F1'][fold, label] += f * 2 * precision * recall / (precision + recall)\n",
    "                    group['scores']['test'][key]['precision'][fold, label] += f * precision\n",
    "                    group['scores']['test'][key]['recall'][fold, label] += f * recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in GROUPS:\n",
    "    for key in group['predictions']:\n",
    "        recalculate_scores(group, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    '#f37500', #(234, 117,   0)\n",
    "#    '#069a2E', #(  6, 154,  46)\n",
    "    '#3465a4', #( 52, 101, 164)\n",
    "#    '#780373', #(120,   3, 115)\n",
    "    '#f10d0c'  #(241,  13,  12)\n",
    "]\n",
    "\n",
    "c = 0\n",
    "for group in GROUPS:\n",
    "    group['color'] = colors[c]\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparative_plot(metric, ax):\n",
    "    for group in GROUPS:\n",
    "        # Create x-values:\n",
    "        x = group[X_KEY]\n",
    "\n",
    "        # Calculate curves:\n",
    "        y_avg = [np.nanmean(group['scores']['test'][n][metric][:,1:]) for n in x]\n",
    "        y_std = [np.nanstd(group['scores']['test'][n][metric][:,1:]) for n in x]\n",
    "\n",
    "        # Plot curves:\n",
    "        ax.errorbar(x, y_avg, yerr=y_std, color=group['color'], label=group['name'], linestyle='--', fmt='o', linewidth=2, capsize=6)\n",
    "\n",
    "    ax.set_xlabel(X_KEY)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "comparative_plot('AUROC', fig.add_subplot(1, 1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "comparative_plot('AUPRC', fig.add_subplot(1, 1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "comparative_plot('F1', fig.add_subplot(1, 1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot recall and precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "comparative_plot('precision', fig.add_subplot(1, 2, 1))\n",
    "comparative_plot('recall', fig.add_subplot(1, 2, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09804e24ad6773f4299ff941abdb533da0618f58a933eb5ec00c0e9780539224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
